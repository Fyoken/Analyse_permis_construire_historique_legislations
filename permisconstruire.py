# -*- coding: utf-8 -*-
"""PermisConstruire.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14xjpvHGKUC-d-sZF44ITIG8qZLtldwsZ

# Import des bibliothèques et connexion au drive
"""

import pandas as pd
import os
from google.colab import drive
import sqlite3
from sqlalchemy import create_engine
import shutil

#drive.mount('/content/gdrive')

"""# Concaténation des fichiers csv en une base SQL"""

# Création d'une base SQL
# Répertoire contenant les fichiers CSV
repertoire_csv = './'

# Liste des fichiers CSV dans le répertoire
fichiers_csv = [f for f in os.listdir(repertoire_csv) if f.endswith('.csv')]

# Créer une connexion à la base de données SQLite
conn = sqlite3.connect('./permisconstruire.db')

# Créer une base de données SQLite
engine = create_engine('sqlite:///permisconstruire.db', echo=False)

# Boucle pour importer chaque fichier CSV dans la base de données
for fichier_csv in fichiers_csv:
    df_temp = pd.read_csv(repertoire_csv+"/"+fichier_csv)
    df_temp.to_sql('permis_construire', con=engine, index=False, if_exists='append')

# Fermer la connexion
engine.dispose()
db_path = './permisconstruire.db'
shutil.copy(db_path, os.path.join(repertoire_csv, 'permisconstruire.db'))

# Fermer la connexion
conn.close()

"""# Vérification des données"""

# Connexion à la base de données SQLite
conn = sqlite3.connect('./permisconstruire.db')

# Requête pour vérifier que tous les fichiers sont présents
query_sample = "SELECT distinct(source) FROM permis_construire LIMIT 30;"

# Exécution de la requête et récupération des résultats dans un DataFrame
df_sample = pd.read_sql(query_sample, conn)

# Afficher les résultats
print(df_sample)

# Fermer la connexion
conn.close()

# Connexion à la base de données SQLite
conn = sqlite3.connect('./permisconstruire.db')

# Requête pour voir à quoi ressemble la base de données
query_sample = "SELECT * FROM permis_construire LIMIT 10;"

# Exécution de la requête et récupération des résultats dans un DataFrame
df_sample = pd.read_sql(query_sample, conn)

# Afficher les résultats
print(df_sample)

# Fermer la connexion
conn.close()

"""# Uniformisation de la colonne de département"""

# Connexion à la base de données SQLite
conn = sqlite3.connect('./permisconstruire.db')
cursor = conn.cursor()

# Exécuter la requête SQL
cursor.execute("""
    UPDATE permis_construire
    SET DEP_COD_DEP =
        CASE
            WHEN LENGTH(CAST(DEP_COD_DEP AS TEXT)) = 1 THEN '00' || DEP_COD_DEP
            WHEN LENGTH(CAST(DEP_COD_DEP AS TEXT)) = 2 THEN '0' || DEP_COD_DEP
            ELSE CAST(DEP_COD_DEP AS TEXT)
        END;
""")

# Valider la mise à jour
conn.commit()

# Fermer la connexion
conn.close()

"""# Exploration des données"""

# Connexion à la base de données SQLite
conn = sqlite3.connect('./permisconstruire.db')

# Requête pour afficher le nombre de bâtiments par année et par département
query_sample = "SELECT PMI_ANNEE_DEP AS Annee, DEP_COD_DEP AS CodeDepartement, COUNT(*) AS nbBat FROM permis_construire WHERE PMI_ANNEE_DEP IS NOT NULL AND DEP_COD_DEP IS NOT NULL GROUP BY PMI_ANNEE_DEP, DEP_COD_DEP ORDER BY PMI_ANNEE_DEP, DEP_COD_DEP;"

# Exécution de la requête et récupération des résultats dans un DataFrame
df_sample = pd.read_sql(query_sample, conn)

# Afficher les résultats
print(df_sample)

# Fermer la connexion
conn.close()

import matplotlib.pyplot as plt

# Pivoter les données pour obtenir un format adapté au graphique
df_pivot = df_sample.pivot(index='Annee', columns='CodeDepartement', values='nbBat')

# Tracer un graphique à barres empilées
df_pivot.plot(kind='bar', stacked=True, figsize=(12, 8))
plt.title('Nombre de bâtiments par année et département')
plt.xlabel('Année')
plt.ylabel('Nombre de Bâtiments')
plt.legend(title='Département', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.show()

# Zoom sur l'île de France
# Connexion à la base de données SQLite
conn = sqlite3.connect('./permisconstruire.db')

# Requête SQL pour compter le nombre de bâtiments par année et en île de France
query = """
    SELECT PMI_ANNEE_DEP AS Annee, COUNT(PMI_ANNEE_DEP) AS nbBat
    FROM permis_construire
    WHERE PMI_ANNEE_DEP IS NOT NULL AND REG_REGION = '11'
    GROUP BY PMI_ANNEE_DEP
    ORDER BY PMI_ANNEE_DEP;
"""

# Exécution de la requête et lecture des résultats dans un DataFrame
df_sample = pd.read_sql_query(query, conn)

# Fermeture de la connexion
conn.close()

# Pivoter les données pour obtenir un format adapté au graphique
df_pivot = df_sample.pivot_table(index='Annee', values='nbBat', aggfunc='sum')

# Tracer un graphique à barres
df_pivot.plot(kind='bar', stacked=True, figsize=(12, 8))
plt.title('Nombre de bâtiments par année en île de France')
plt.xlabel('Année')
plt.ylabel('Nombre de Bâtiments')
plt.show()

"""# Exploration du nombre d'étages"""

# Connexion à la base de données SQLite
conn = sqlite3.connect('./permisconstruire.db')

# Requête pour afficher le nombre d'étages dans les bâtiments par année et par département
query_sample = "SELECT PMI_ANNEE_DEP AS Annee, DEP_COD_DEP AS CodeDepartement, PHR_NB_NIVMAX as nbetages FROM permis_construire WHERE PMI_ANNEE_DEP IS NOT NULL AND DEP_COD_DEP IS NOT NULL GROUP BY PMI_ANNEE_DEP, DEP_COD_DEP ORDER BY PMI_ANNEE_DEP, DEP_COD_DEP;"

# Exécution de la requête et récupération des résultats dans un DataFrame
df_sample = pd.read_sql(query_sample, conn)

# Afficher les résultats
print(df_sample)

# Fermer la connexion
conn.close()

# Pivoter les données pour obtenir un format adapté au graphique
df_pivot = df_sample.pivot(index='Annee', columns='CodeDepartement', values='nbetages')

# Tracer un graphique à barres
df_pivot.plot(kind='bar', stacked=True, figsize=(12, 8))
plt.title('Nombre d\'étages des bâtiments par année et département')
plt.xlabel('Année')
plt.ylabel('Nombre d\'étages')
plt.legend(title='Département', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.show()

df2 = 2.85*df_pivot
df2.plot(kind="bar", stacked=True, figsize=(12, 8))
plt.title('Hauteur des bâtiments par année et département')
plt.xlabel('Année')
plt.ylabel('Hauteur')
plt.legend(title='Département', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.show()

"""# Hauteur moyenne des bâtiments en Ile de France"""

# Connexion à la base de données SQLite
conn = sqlite3.connect('./permisconstruire.db')

# Requête pour afficher la hauteur dans les bâtiments par année en Ile de France
query_sample = "SELECT PMI_ANNEE_DEP AS Annee, DEP_COD_DEP AS CodeDepartement, PHR_NB_NIVMAX as hauteur FROM permis_construire WHERE PMI_ANNEE_DEP IS NOT NULL AND DEP_COD_DEP IS NOT NULL AND REG_REGION = '11' GROUP BY PMI_ANNEE_DEP, DEP_COD_DEP ORDER BY PMI_ANNEE_DEP, DEP_COD_DEP;"

# Exécution de la requête et récupération des résultats dans un DataFrame
df_sample = pd.read_sql(query_sample, conn)

# Afficher les résultats
print(df_sample)

# Fermer la connexion
conn.close()

# Pivoter les données pour obtenir un format adapté au graphique
df_pivot = 2.85*df_sample.pivot_table(index='Annee', values='hauteur', aggfunc='sum')

# Tracer un graphique à barres
df_pivot.plot(kind='bar', stacked=True, figsize=(12, 8))
plt.title('Hauteur des bâtiments par année en île de France')
plt.xlabel('Année')
plt.ylabel('Hauteur')
plt.show()

"""# Coefficient de prise au sol"""

# Connexion à la base de données SQLite
conn = sqlite3.connect('./permisconstruire.db')

query_sample = """
SELECT
    PMI_ANNEE_DEP AS Annee,
    DEP_COD_DEP AS CodeDepartement,
    PHR_LI1_SHONANT AS Shonant1,
    PHR_LI2_SHONANT AS Shonant2,
    PHR_LI3_SHONANT AS Shonant3,
    PHR_LI4_SHONANT AS Shonant4,
    PHR_LI5_SHONANT AS Shonant5,
    PHR_LI6_SHONANT AS Shonant6,
    PHR_LI7_SHONANT AS Shonant7,
    PHR_LI8_SHONANT AS Shonant8,
    PHR_LI9_SHONANT AS Shonant9,
    PHR_LI1_SHONCR AS Shoncr1,
    PHR_LI2_SHONCR AS Shoncr2,
    PHR_LI3_SHONCR AS Shoncr3,
    PHR_LI4_SHONCR AS Shoncr4,
    PHR_LI5_SHONCR AS Shoncr5,
    PHR_LI6_SHONCR AS Shoncr6,
    PHR_LI7_SHONCR AS Shoncr7,
    PHR_LI8_SHONCR AS Shoncr8,
    PHR_LI9_SHONCR AS Shoncr9,
    PHR_LI1_SHONDEM AS Shondem1,
    PHR_LI2_SHONDEM AS Shondem2,
    PHR_LI3_SHONDEM AS Shondem3,
    PHR_LI4_SHONDEM AS Shondem4,
    PHR_LI5_SHONDEM AS Shondem5,
    PHR_LI6_SHONDEM AS Shondem6,
    PHR_LI7_SHONDEM AS Shondem7,
    PHR_LI8_SHONDEM AS Shondem8,
    PHR_LI9_SHONDEM AS Shondem9

FROM permis_construire
WHERE PMI_ANNEE_DEP IS NOT NULL AND DEP_COD_DEP IS NOT NULL
GROUP BY PMI_ANNEE_DEP, DEP_COD_DEP
ORDER BY PMI_ANNEE_DEP, DEP_COD_DEP;
"""

# Exécution de la requête et récupération des résultats dans un DataFrame
df_result = pd.read_sql(query_sample, conn)

# Afficher les résultats
print(df_result)

# Fermer la connexion
conn.close()

# Calculer la somme spécifiée (SHONANT + SHONCR - SHONDEM)

df_result['Sum_Shonant'] = df_result['Shonant1'] + df_result['Shonant2'] + df_result['Shonant3'] + df_result['Shonant4'] + df_result['Shonant5'] + df_result['Shonant6'] + df_result['Shonant7'] + df_result['Shonant8'] + df_result['Shonant9']
df_result['Sum_Shoncr'] = df_result['Shoncr1'] + df_result['Shoncr2'] + df_result['Shoncr3'] + df_result['Shoncr4'] + df_result['Shoncr5'] + df_result['Shoncr6'] + df_result['Shoncr7'] + df_result['Shoncr8'] + df_result['Shoncr9']
df_result['Sum_Shondem'] = df_result['Shondem1'] + df_result['Shondem2'] + df_result['Shondem3'] + df_result['Shondem4'] + df_result['Shondem5'] + df_result['Shondem6'] + df_result['Shondem7'] + df_result['Shondem8'] + df_result['Shondem9']

df_result['Result'] = df_result['Sum_Shonant'] + df_result['Sum_Shoncr'] - df_result['Sum_Shondem']

# Afficher le DataFrame résultant
print(df_result)

import matplotlib.pyplot as plt

moyenne_par_annee_departement = df_result.groupby(['Annee', 'CodeDepartement'])['Result'].mean().reset_index()

# Pivoter les données pour obtenir un format adapté au graphique
df_pivot = moyenne_par_annee_departement.pivot(index='Annee', columns='CodeDepartement', values='Result')

# Tracer un graphique à barres
df_pivot.plot(kind='bar', stacked=True, figsize=(12, 8))
plt.title('Moyenne des surfaces des batiments par année et département')
plt.xlabel('Année')
plt.ylabel('Moyenne des résultats')
plt.legend(title='Département', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.show()

"""# Superficie Terrain"""

import matplotlib.pyplot as plt
# Connexion à la base de données SQLite
conn = sqlite3.connect('./permisconstruire.db')

# Requête pour afficher le coef de prise au sol dans les bâtiments par année et par département
query_sample = "SELECT PMI_ANNEE_DEP AS Annee, DEP_COD_DEP AS CodeDepartement, PHR_SUPERFICIE_TERR as coef FROM permis_construire WHERE PMI_ANNEE_DEP IS NOT NULL AND DEP_COD_DEP IS NOT NULL GROUP BY PMI_ANNEE_DEP, DEP_COD_DEP ORDER BY PMI_ANNEE_DEP, DEP_COD_DEP;"

# Exécution de la requête et récupération des résultats dans un DataFrame
df_sample = pd.read_sql(query_sample, conn)

# Afficher les résultats
print(df_sample)

# Fermer la connexion
conn.close()

# Pivoter les données pour obtenir un format adapté au graphique
df_pivot = df_sample.pivot(index='Annee', columns='CodeDepartement', values='coef')

# Tracer un graphique à barres
df_pivot.plot(kind='bar', stacked=True, figsize=(12, 8))
plt.title('Superficie au sol par année')
plt.xlabel('Année')
plt.ylabel('Superficie au sol')
plt.legend(title='Département', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.show()

df2 = 2.85*df_pivot
df2.plot(kind="bar", stacked=True, figsize=(12, 8))
plt.title('Superficie au sol par année et département')
plt.xlabel('Année')
plt.ylabel('Superficie')
plt.legend(title='Département', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.show()

"""# Distribution de la hauteur des bâtiments par commune au cours du temps et première détection de bunching"""

url_correspondance_communes = './v_commune_2023.csv'
df_correspondance_communes = pd.read_csv(url_correspondance_communes, sep=',')

# Connexion à la base de données SQLite
conn = sqlite3.connect('./permisconstruire.db')

# Requête pour afficher la distribution de la hauteur des bâtiments par année dans chaque commune
query_sample = """
SELECT
    PHR_NB_NIVMAX As NombreEtages,
    PMI_ANNEE_DEP AS Annee,
    DEP_COD_DEP AS CodeDepartement,
    COM_COD_INSEE AS CodeCommune,
    PHR_LI1_SHONCR AS Shoncr1,
    PHR_LI1_SHON2CR AS Shon2cr1
FROM permis_construire
WHERE PMI_ANNEE_DEP IS NOT NULL AND DEP_COD_DEP IS NOT NULL AND (PHR_LI1_SHONCR IS NOT NULL OR PHR_LI1_SHON2CR IS NOT NULL)
GROUP BY PMI_ANNEE_DEP, DEP_COD_DEP, COM_COD_INSEE, PHR_LI1_SHONANT
ORDER BY PMI_ANNEE_DEP, DEP_COD_DEP;
"""

# Exécution de la requête et récupération des résultats dans un DataFrame
df_sample = pd.read_sql(query_sample, conn)

# Fermer la connexion
conn.close()

# Fusionner les DataFrames pour ajouter le nom de la commune
df_merged = pd.merge(df_sample, df_correspondance_communes, left_on='CodeCommune', right_on='COM')

def remplir_format_annee(yy):
    if yy >= 0 and yy <= 21:
        return 2000 + yy
    else:
        return 1900 + yy

import matplotlib.pyplot as plt
import numpy as np

df_merged['CodeCommune'] = df_merged['CodeCommune'].apply(lambda x: f'0{x}' if len(str(x)) == 4 else str(x))
df_merged['Annee'] = df_merged['Annee'].apply(remplir_format_annee)

for commune, data in df_merged.groupby(['LIBELLE']):
    if len(data) >= 25:
        # Extraction des données pour le tracé
        annees = data['Annee']
        nb_etages = data['NombreEtages']

        # Filtrer les données pour la tranche de 10 ans (2010-2019)
        data_2010_2019 = data[(annees >= 2010) & (annees <= 2019)]
        data_2010_2019 = data_2010_2019.dropna(subset=['NombreEtages'])
        if not data_2010_2019.empty:
          # Calculer l'histogramme
          hist, bins = np.histogram(data_2010_2019['NombreEtages'], bins=range(int(data_2010_2019['NombreEtages'].min()), int(data_2010_2019['NombreEtages'].max()) + 2), density=True)

          # Vérifier la différence significative positive entre les hauteurs des bins
          significant_increase = any(np.diff(hist) > 0.5)
          significant_decrease = any(np.diff(hist) < -0.5)

          # Afficher l'histogramme uniquement si la difference est significative
          if (significant_increase or significant_decrease) and data_2010_2019['NombreEtages'].max() >= 3:
            plt.figure(figsize=(10, 6))
            plt.bar(bins[:-1], hist, width=1, align='edge', color='skyblue', edgecolor='black')
            plt.xlabel('Nombre d\'étages')
            plt.ylabel(f'Densité dans la Commune {commune}')
            plt.title(f'Distribution du nombre d\'étages des Logements dans la Commune {commune} (2010-2019)')
            plt.show()

for commune, data in df_merged.groupby(['LIBELLE']):
    if len(data) >= 25:
        # Extraction des données pour le tracé
        annees = data['Annee']
        nb_etages = data['NombreEtages']

        # Filtrer les données pour la tranche de 10 ans (2000-2009)
        data_2000_2009 = data[(annees >= 2000) & (annees <= 2009)]
        data_2000_2009 = data_2000_2009.dropna(subset=['NombreEtages'])
        if not data_2000_2009.empty:
          # Calculer l'histogramme
          hist, bins = np.histogram(data_2000_2009['NombreEtages'], bins=range(int(data_2000_2009['NombreEtages'].min()), int(data_2000_2009['NombreEtages'].max()) + 2), density=True)

          # Vérifier la différence significative positive entre les hauteurs des bins
          significant_increase = any(np.diff(hist) > 0.5)
          significant_decrease = any(np.diff(hist) < -0.5)

          # Afficher l'histogramme uniquement si la difference est significative
          if (significant_increase or significant_decrease) and data_2000_2009['NombreEtages'].max() >= 3:
            plt.figure(figsize=(10, 6))
            plt.bar(bins[:-1], hist, width=1, align='edge', color='skyblue', edgecolor='black')
            plt.xlabel('Nombre d\'étages')
            plt.ylabel(f'Densité dans la Commune {commune}')
            plt.title(f'Distribution du nombre d\'étages des Logements dans la Commune {commune} (2000-2009)')
            plt.show()

for commune, data in df_merged.groupby(['LIBELLE']):
    if len(data) >= 25:
        # Extraction des données pour le tracé
        annees = data['Annee']
        nb_etages = data['NombreEtages']

        # Filtrer les données pour la tranche de 10 ans (1990-1999)
        data_1990_1999 = data[(annees >= 1990) & (annees <= 1999)]
        data_1990_1999 = data_1990_1999.dropna(subset=['NombreEtages'])

        if not data_1990_1999.empty:
          # Calculer l'histogramme
          hist, bins = np.histogram(data_1990_1999['NombreEtages'], bins=range(int(data_1990_1999['NombreEtages'].min()), int(data_1990_1999['NombreEtages'].max()) + 2), density=True)

          # Vérifier la différence significative positive entre les hauteurs des bins
          significant_increase = any(np.diff(hist) > 0.5)
          significant_decrease = any(np.diff(hist) < -0.5)

          # Afficher l'histogramme uniquement si la difference est significative
          if (significant_increase or significant_decrease) and data_1990_1999['NombreEtages'].max() >= 3:
            plt.figure(figsize=(10, 6))
            plt.bar(bins[:-1], hist, width=1, align='edge', color='skyblue', edgecolor='black')
            plt.xlabel('Nombre d\'étages')
            plt.ylabel(f'Densité dans la Commune {commune}')
            plt.title(f'Distribution du nombre d\'étages des Logements dans la Commune {commune} (1990-1999)')
            plt.show()

"""# Remise en question de la variable nombre d'étages : vérification de la validité par l'étude des logements collectifs"""

import matplotlib.pyplot as plt
url_correspondance_communes = './v_commune_2023.csv'
df_correspondance_communes = pd.read_csv(url_correspondance_communes, sep=',')

# Connexion à la base de données SQLite
conn = sqlite3.connect('./permisconstruire.db')

# Requête pour afficher la distribution de la hauteur des bâtiments par année dans chaque commune
query_sample = """
SELECT
    PHR_NB_NIVMAX As NombreEtages,
    PMI_ANNEE_DEP AS Annee,
    DEP_COD_DEP AS CodeDepartement,
    COM_COD_INSEE AS CodeCommune,
    PHR_LI1_SHONCR AS Shoncr1,
    PHR_LI1_SHON2CR AS Shon2cr1,
    PMI_TER_NUM_VOIE as Num,
    PMI_TER_TYP_VOIE as Type,
    PMI_TER_LIB_VOIE as Libelle,
    PMI_TER_COD_POST as CodePostal
FROM permis_construire
WHERE TPE_COD_TYPPERMIS="PC" AND NAP_COD_NATPROJ=1 AND PMI_ANNEE_DEP IS NOT NULL AND DEP_COD_DEP IS NOT NULL AND PHR_AUT_SHON_LOGIP IS NULL AND PHR_AUT_SHON_LOGCOL > 0 AND (PHR_LI1_SHONCR > PHR_SUPERFICIE_TERR OR PHR_LI1_SHON2CR > PHR_SUPERFICIE_TERR)
GROUP BY PMI_ANNEE_DEP, DEP_COD_DEP, COM_COD_INSEE, PHR_LI1_SHONANT
ORDER BY PMI_ANNEE_DEP, DEP_COD_DEP;
"""

# Exécution de la requête et récupération des résultats dans un DataFrame
df_sample = pd.read_sql(query_sample, conn)

# Fermer la connexion
conn.close()

# Fusionner les DataFrames pour ajouter le nom de la commune
df_merged = pd.merge(df_sample, df_correspondance_communes, left_on='CodeCommune', right_on='COM')

import matplotlib.pyplot as plt
import pandas as pd

# Filtrer les données pour exclure les entrées avec des valeurs manquantes dans PHR_NB_NIVMAX
df_filtered = df_merged.dropna(subset=['NombreEtages'])

# Logements collectifs : 2585 / 8861 avec 0 étage
# Logements individuels : 9062 / 12739 avec 0 étage
nombre_total_permis = df_merged.shape[0]
print("Nombre total de permis de construire :", nombre_total_permis)
# Nombre de permis avec 0 étage
nombre_permis_zero_etage = df_merged[df_merged['NombreEtages'] == 0].shape[0]
print("Nombre de permis avec 0 étage :", nombre_permis_zero_etage)


# Analyse de la distribution des nombres d'étages (avec les zéros)
plt.figure(figsize=(10, 6))
plt.hist(df_filtered['NombreEtages'], bins=range(int(df_filtered['NombreEtages'].min()), int(df_filtered['NombreEtages'].max()) + 2), color='skyblue', edgecolor='black', label='Avec 0')
plt.xlabel('Nombre d\'étages')
plt.ylabel('Fréquence')
plt.title('Distribution du nombre d\'étages des logements collectifs')
plt.grid(True)
plt.xlim(0, 5)  # Utilisation de plt.xlim pour définir les limites des axes x
plt.legend()
plt.show()

import numpy as np
import matplotlib.pyplot as plt

df_merged['CodeCommune'] = df_merged['CodeCommune'].apply(lambda x: f'0{x}' if len(str(x)) == 4 else str(x))
df_merged['Annee'] = df_merged['Annee'].apply(remplir_format_annee)

# Nombre total de permis de construire avant le filtrage
nombre_permis_avant = df_merged.shape[0]

# 1. Filtrer les lignes où l'adresse n'est pas nulle
# Supprime 7049 permis de construire sur 8861
#df_merged = df_merged.dropna(subset=['Num', 'Type', 'Libelle', 'CodePostal'])

# 2. Obtenir le permis de construire avec le nombre d'étages le plus élevé pour chaque adresse renseignée
# Supprime 61 permis de construire sur 8861
# Filtrer les lignes où l'adresse n'est pas None
df_merged_non_none = df_merged.dropna(subset=['Num', 'Type', 'Libelle', 'CodePostal'])

# Appliquer la suppression des doublons uniquement aux lignes non None
df_merged_non_none = df_merged_non_none.loc[df_merged_non_none.groupby(['Num', 'Type', 'Libelle', 'CodePostal'])['NombreEtages'].idxmax()]

# Concaténer les résultats avec les lignes où l'adresse est None
df_merged = pd.concat([df_merged_non_none, df_merged[df_merged[['Num', 'Type', 'Libelle', 'CodePostal']].isnull().any(axis=1)]])

# Nombre total de permis de construire après le filtrage
nombre_permis_apres = df_merged.shape[0]

print(f"Nombre total de permis de construire avant le filtrage : {nombre_permis_avant}")
print(f"Nombre total de permis de construire après le filtrage : {nombre_permis_apres}")

for commune, data in df_merged.groupby(['LIBELLE']):
    if len(data) >= 25:
        # Extraction des données pour le tracé
        annees = data['Annee']
        nb_etages = data['NombreEtages']

        # Filtrer les données pour la tranche de 10 ans (2010-2019)
        data_2010_2019 = data[(annees >= 2010) & (annees <= 2019)]
        data_2010_2019 = data_2010_2019.dropna(subset=['NombreEtages'])
        if not data_2010_2019.empty:
            # Calculer l'histogramme
            hist, bins = np.histogram(data_2010_2019['NombreEtages'], bins=range(int(data_2010_2019['NombreEtages'].min()), int(data_2010_2019['NombreEtages'].max()) + 1), density=True)

            # Imprimer quelques-unes des données où la distribution est à 100% à 0
            if len(hist) > 0 and hist[0] >= 0.8:
                print(f"Commune: {commune}")
                print("Données où la distribution est à 80% à 0:")
                count = 0
                for index, row in data_2010_2019[(data_2010_2019['NombreEtages'] == 0) & (~data_2010_2019['Num'].isnull())].iterrows():
                    print(f"Num: {row['Num']}, Type: {row['Type']}, Libelle: {row['Libelle']}, CodePostal: {row['CodePostal']}")
                    count += 1
                    if count == 5:  # Limite de 2 par commune
                        break

            # Tracé de l'histogramme
            plt.figure(figsize=(10, 6))
            plt.bar(bins[:-1], hist, width=1, align='edge', color='skyblue', edgecolor='black')
            plt.xlabel('Nombre d\'étages')
            plt.ylabel(f'Densité dans la Commune {commune}')
            plt.title(f'Distribution du nombre d\'étages des logements collectifs dans la commune {commune} (2010-2019)')
            plt.show()

import numpy as np

df_merged['CodeCommune'] = df_merged['CodeCommune'].apply(lambda x: f'0{x}' if len(str(x)) == 4 else str(x))
df_merged['Annee'] = df_merged['Annee'].apply(remplir_format_annee)

for commune, data in df_merged.groupby(['LIBELLE']):
    if len(data) >= 25:
        # Extraction des données pour le tracé
        annees = data['Annee']
        nb_etages = data['NombreEtages']

        # Filtrer les données pour la tranche de 10 ans (2010-2019)
        data_2010_2019 = data[(annees >= 2010) & (annees <= 2019)]
        data_2010_2019 = data_2010_2019.dropna(subset=['NombreEtages'])
        if not data_2010_2019.empty:
          # Calculer l'histogramme
          hist, bins = np.histogram(data_2010_2019['NombreEtages'], bins=range(int(data_2010_2019['NombreEtages'].min()), int(data_2010_2019['NombreEtages'].max()) + 1), density=True)

          # Vérifier la différence significative positive entre les hauteurs des bins
          significant_increase = any(np.diff(hist) > 0.5)
          significant_decrease = any(np.diff(hist) < -0.5)

          # Afficher l'histogramme uniquement si la difference est significative et qu'il y a suffisamment d'étages
          if (significant_increase or significant_decrease) and data_2010_2019['NombreEtages'].max() >= 3:
            plt.figure(figsize=(10, 6))
            plt.bar(bins[:-1], hist, width=1, align='edge', color='skyblue', edgecolor='black')
            plt.xlabel('Nombre d\'étages')
            plt.ylabel(f'Densité dans la Commune {commune}')
            plt.title(f'Distribution du nombre d\'étages des Logements dans la Commune {commune} (2010-2019)')
            plt.show()

"""Après vérification géospatiale sur Google Maps, la majorité des adresses présentant zéro étage par notre variable, a en réalité bien plus d'étages ou bien est invérifiable car l'adresse n'est pas renseignée.

# Distribution et Bunchings de la superficie du terrain
"""

import sqlite3
import matplotlib.pyplot as plt

# Connexion à la base de données
conn = sqlite3.connect('./permisconstruire.db')

# Exécution de la requête SQL pour extraire les données
query = """
    SELECT
        PHR_AT_ANRE AS annee,
        AVG(PHR_SUPERFICIE_TERR) as superficie_moyenne
    FROM
        permis_construire
    WHERE
        PHR_AT_ANRE IS NOT NULL AND DEP_COD_DEP IS NOT NULL
    GROUP BY
        PHR_AT_ANRE
    ORDER BY
        PHR_AT_ANRE;
"""

# Exécution de la requête et récupération des résultats
cursor = conn.execute(query)
rows = cursor.fetchall()

# Extraction des données pour le tracé
annees = [row[0] for row in rows]
superficies_moyennes = [row[1] for row in rows]

# Visualisation des résultats
plt.plot(annees, superficies_moyennes, marker='o')
plt.xlabel('Année')
plt.ylabel('Superficie Moyenne Globale')
plt.title('Superficie Moyenne Globale des Bâtiments au fil des Années')
plt.show()

# Fermer la connexion
conn.close()

import sqlite3
import matplotlib.pyplot as plt

# Connexion à la base de données
conn = sqlite3.connect('./permisconstruire.db')

# Département spécifique que vous souhaitez visualiser
departement_cible = '060'

# Exécution de la requête SQL pour extraire les données
query = f"""
    SELECT
        PHR_AT_ANRE AS annee,
        AVG(PHR_SUPERFICIE_TERR) as superficie_moyenne
    FROM
        permis_construire
    WHERE
        PHR_AT_ANRE IS NOT NULL AND DEP_COD_DEP = '{departement_cible}'
    GROUP BY
        PHR_AT_ANRE
    ORDER BY
        PHR_AT_ANRE;
"""

# Exécution de la requête et récupération des résultats
cursor = conn.execute(query)
rows = cursor.fetchall()

# Extraction des données pour le tracé
annees = [row[0] for row in rows]
superficies_moyennes = [row[1] for row in rows]

# Visualisation des résultats
plt.plot(annees, superficies_moyennes, marker='o')
plt.xlabel('Année')
plt.ylabel(f'Superficie Moyenne dans le Département {departement_cible}')
plt.title(f'Superficie Moyenne Globale des Bâtiments dans le Département {departement_cible} au fil des Années')
plt.show()

# Fermer la connexion
conn.close()

import sqlite3
import matplotlib.pyplot as plt

# Connexion à la base de données
conn = sqlite3.connect('./permisconstruire.db')

# Code INSEE de la commune spécifique que vous souhaitez visualiser
code_insee_cible = '60057'

# Exécution de la requête SQL pour extraire les données
query = f"""
    SELECT
        PHR_AT_ANRE AS annee,
        AVG(PHR_SUPERFICIE_TERR) as superficie_moyenne
    FROM
        permis_construire
    WHERE
        PHR_AT_ANRE IS NOT NULL AND COM_COD_INSEE = '{code_insee_cible}'
    GROUP BY
        PHR_AT_ANRE
    ORDER BY
        PHR_AT_ANRE;
"""

# Exécution de la requête et récupération des résultats
cursor = conn.execute(query)
rows = cursor.fetchall()

# Extraction des données pour le tracé
annees = [row[0] for row in rows]
superficies_moyennes = [row[1] for row in rows]

# Visualisation des résultats
plt.plot(annees, superficies_moyennes, marker='o')
plt.xlabel('Année')
plt.ylabel(f'Superficie Moyenne dans la Commune {code_insee_cible}')
plt.title(f'Superficie Moyenne Globale des Bâtiments dans la Commune {code_insee_cible} au fil des Années')
plt.show()

# Fermer la connexion
conn.close()

"""# Code Habitations"""

# Connexion à la base de données SQLite
conn = sqlite3.connect('./ING3/PFE/permisconstruire.db')

query_sample = """
SELECT
    PMI_ANNEE_DEP AS Annee,
    DEP_COD_DEP AS CodeDepartement,
    COM_COD_INSEE AS CodeCommune,
    PHR_NB_NIVMAX as NombreEtages,
    PHR_LI1_SHONCR AS Shoncr1,
    PHR_LI1_SHON2CR AS Shon2cr1
FROM permis_construire
WHERE PMI_ANNEE_DEP IS NOT NULL AND DEP_COD_DEP IS NOT NULL
GROUP BY PMI_ANNEE_DEP, DEP_COD_DEP, COM_COD_INSEE, PHR_LI1_SHONANT
ORDER BY PMI_ANNEE_DEP, DEP_COD_DEP;
"""

# Exécution de la requête et récupération des résultats dans un DataFrame
df_priseAuSol = pd.read_sql(query_sample, conn)

# Afficher les résultats
print(df_priseAuSol)

# Fermer la connexion
conn.close()

def remplir_format_annee(yy):
    if yy >= 0 and yy <= 21:
        return 2000 + yy
    else:
        return 1900 + yy

#vérifie que cela possède un étage
df_priseAuSol_nonNul = df_priseAuSol.loc[df_priseAuSol["NombreEtages"]!=0]
#ne prends pas en compte les choses qui ne sont pas habitations
df_priseAuSol_nonNul = df_priseAuSol_nonNul.loc[(df_priseAuSol_nonNul["Shoncr1"]!=0 ) | (df_priseAuSol_nonNul["Shoncr1"]!=0)]
#avoir le nombre de m2 et m2 par étage
df_priseAuSol_nonNul["Somme"] = (df_priseAuSol_nonNul["Shoncr1"] + df_priseAuSol_nonNul["Shon2cr1"])
df_priseAuSol_nonNul["SurfaceMoyEtage"] = (df_priseAuSol_nonNul["Somme"])/df_priseAuSol_nonNul["NombreEtages"]
df_priseAuSol_nonNul["Annee"] = df_priseAuSol_nonNul['Annee'].apply(remplir_format_annee)
df_priseAuSol_nonNul = df_priseAuSol_nonNul.sort_values(by='Annee', ascending=True)

print(df_priseAuSol_nonNul)

#on s'occupe d'une commune en particulier
surface_max = df_priseAuSol_nonNul["Somme"].max()
commune_cible = "94068"

df_SaintMaure = df_priseAuSol_nonNul[df_priseAuSol_nonNul['CodeCommune'] == commune_cible]

print(df_SaintMaure)

resultats = []

intervalle_1 = (0, 50)
intervalle_2 = (50, 100)
intervalle_3 = (100, 150)
intervalle_4 = (150, 200)
intervalle_5 = (200, 50000)

for annee in df_SaintMaure['Annee'].unique():
    # Filtrer le DataFrame pour l'année spécifique
    df_annee = df_SaintMaure[df_SaintMaure['Annee'] == annee]

    # Diviser les données en intervalles
    df_annee['Intervalle'] = pd.cut(df_annee['Somme'], bins=[intervalle_1[0], intervalle_1[1], intervalle_2[1], intervalle_3[1], intervalle_4[1], intervalle_5[1]], right=False, labels=['0-50', '50-100', '100-150',' 150-200' ,'200++'])

    # Compter les occurrences dans chaque intervalle
    compte_0_50 = df_annee[df_annee['Intervalle'] == '0-50'].shape[0]
    compte_50_100 = df_annee[df_annee['Intervalle'] == '50-100'].shape[0]
    compte_100_150 = df_annee[df_annee['Intervalle'] == '100-150'].shape[0]
    compte_150_200 = df_annee[df_annee['Intervalle'] == '150-200'].shape[0]
    compte_200_plus = df_annee[df_annee['Intervalle'] == '200++'].shape[0]

    # Ajouter les tuples à la liste
    resultats.append((annee, '0-50', compte_0_50))
    resultats.append((annee, '50-100', compte_50_100))
    resultats.append((annee, '100-150', compte_100_150))
    resultats.append((annee, '150-200', compte_150_200))
    resultats.append((annee, '200++', compte_200_plus))

print("\nListe de tuples résultante :")
print(resultats)

#affichage de la proportion par tranche

import matplotlib.pyplot as plt
# Transformer la liste de tuples en DataFrame pour faciliter la manipulation
df_resultats = pd.DataFrame(resultats, columns=['Annee', 'Intervalle', 'Compte'])

# Calculer les proportions par tranches
df_resultats['Proportion'] = df_resultats.groupby('Annee')['Compte'].transform(lambda x: x / x.sum())

# Afficher le graphique à barres
fig, ax = plt.subplots(figsize=(10, 6))

# Utiliser seaborn pour des couleurs agréables
import seaborn as sns
sns.set(style="whitegrid")

# Tracer le graphique à barres
sns.barplot(x='Annee', y='Proportion', hue='Intervalle', data=df_resultats, ax=ax)

# Ajouter des labels et un titre
ax.set(xlabel='Année', ylabel='Proportion', title='Proportion par tranches')

# Afficher la légende
ax.legend(title='Intervalle')

# Afficher le graphique
plt.show()

#affichage du nombre par tranche

# Transformer la liste de tuples en DataFrame pour faciliter la manipulation
df_resultats = pd.DataFrame(resultats, columns=['Annee', 'Intervalle', 'Compte'])

# Afficher le nuage de points reliés pour la proportion
fig, ax1 = plt.subplots(figsize=(10, 6))

# Boucler à travers les intervalles et tracer les points reliés par des lignes
for intervalle in df_resultats['Intervalle'].unique():
    df_intervalle = df_resultats[df_resultats['Intervalle'] == intervalle]
    ax1.plot(df_intervalle['Annee'], df_intervalle['Compte'], marker='o', label=intervalle)

# Ajouter des labels et un titre
ax1.set(xlabel='Année', ylabel='Nombre', title='Nombre par tranches')

# Afficher la légende
ax1.legend(title='Intervalle')

# Afficher le graphique
plt.show()

import sqlite3
import matplotlib.pyplot as plt
import numpy as np

# Connexion à la base de données
conn = sqlite3.connect('./permisconstruire.db')

# Exécution de la requête SQL pour extraire les données
query = """
    SELECT
        PHR_AT_ANRE AS annee,
        AVG(PHR_SUPERFICIE_TERR) as superficie_moyenne
    FROM
        permis_construire
    WHERE
        PHR_AT_ANRE IS NOT NULL AND DEP_COD_DEP IS NOT NULL
    GROUP BY
        PHR_AT_ANRE
    ORDER BY
        PHR_AT_ANRE;
"""

# Exécution de la requête et récupération des résultats
cursor = conn.execute(query)
rows = cursor.fetchall()

# Extraction des données pour le tracé
annees = [row[0] for row in rows]
superficies_moyennes = [row[1] for row in rows]

# Diviser les années en tranches de 10 ans
bins = np.arange(min(annees), max(annees) + 10, 10)

# Associer chaque valeur de superficie à la tranche de 10 ans correspondante
digitized = np.digitize(annees, bins)
binned_superficies = {i: [] for i in range(1, len(bins))}
for i in range(len(annees)):
    binned_superficies[digitized[i]].append(superficies_moyennes[i])

# Calculer la moyenne pour chaque tranche
avg_superficies_by_bin = [np.mean(binned_superficies[i]) for i in range(1, len(bins))]

# Visualisation des résultats sous forme d'histogramme
plt.bar(bins[:-1], avg_superficies_by_bin, width=10, align='edge')
plt.xlabel('Tranche de 10 ans')
plt.ylabel('Superficie Moyenne Globale')
plt.title('Superficie Moyenne Globale des Bâtiments par Tranche de 10 ans')
plt.show()

# Fermer la connexion
conn.close()

import sqlite3
import matplotlib.pyplot as plt
import numpy as np

# Connexion à la base de données
conn = sqlite3.connect('./permisconstruire.db')

# Code INSEE de la commune spécifique que vous souhaitez visualiser
code_insee_cible = '60057'

# Exécution de la requête SQL pour extraire les données
query = f"""
    SELECT
        PHR_AT_ANRE AS annee,
        PHR_SUPERFICIE_TERR as superficie
    FROM
        permis_construire
    WHERE
        PHR_AT_ANRE IS NOT NULL AND COM_COD_INSEE = '{code_insee_cible}'
    ORDER BY
        PHR_AT_ANRE;
"""

# Exécution de la requête et récupération des résultats
cursor = conn.execute(query)
rows = cursor.fetchall()

# Extraction des données pour le tracé
annees = [row[0] for row in rows]
superficies = [row[1] for row in rows]

# Diviser les années en tranches de 10 ans
bins = np.arange(min(annees), max(annees) + 10, 10)

# Associer chaque valeur de superficie à la tranche de 10 ans correspondante
digitized = np.digitize(annees, bins)
binned_superficies = {i: [] for i in range(1, len(bins))}
for i in range(len(annees)):
    binned_superficies[digitized[i]].append(superficies[i])

# Calculer la moyenne pour chaque tranche
avg_superficies_by_bin = [np.mean(binned_superficies[i]) for i in range(1, len(bins))]

# Visualisation des résultats sous forme d'histogramme
plt.bar(bins[:-1], avg_superficies_by_bin, width=10, align='edge')
plt.xlabel('Tranche de 10 ans')
plt.ylabel(f'Superficie Moyenne dans la Commune {code_insee_cible}')
plt.title(f'Superficie Moyenne des Bâtiments dans la Commune {code_insee_cible} par Tranche de 10 ans')
plt.show()

# Fermer la connexion
conn.close()

import sqlite3
import matplotlib.pyplot as plt
import numpy as np

# Connexion à la base de données
conn = sqlite3.connect('./permisconstruire.db')

# Département spécifique que vous souhaitez visualiser
departement_cible = '060'  # Remplacez '60' par le code de votre département

# Exécution de la requête SQL pour extraire les données
query = f"""
    SELECT
        PHR_AT_ANRE AS annee,
        PHR_SUPERFICIE_TERR as superficie
    FROM
        permis_construire
    WHERE
        PHR_AT_ANRE IS NOT NULL AND DEP_COD_DEP = '{departement_cible}'
    ORDER BY
        PHR_AT_ANRE;
"""

# Exécution de la requête et récupération des résultats
cursor = conn.execute(query)
rows = cursor.fetchall()

# Extraction des données pour le tracé
annees = [row[0] for row in rows]
superficies = [row[1] for row in rows]

# Diviser les années en tranches de 10 ans
bins = np.arange(min(annees), max(annees) + 10, 10)

# Associer chaque valeur de superficie à la tranche de 10 ans correspondante
digitized = np.digitize(annees, bins)

# Initialiser le dictionnaire avec des listes vides pour toutes les clés possibles
binned_superficies = {i: [] for i in range(1, len(bins) + 1)}

# Ajouter chaque valeur de superficie à la tranche de 10 ans correspondante
for i in range(len(annees)):
    bin_index = digitized[i]
    if bin_index in binned_superficies:
        binned_superficies[bin_index].append(superficies[i])

# Calculer la moyenne pour chaque tranche
avg_superficies_by_bin = [np.mean(binned_superficies[i]) for i in range(1, len(bins) + 1)]

# Vérifier et ajuster la longueur de bins si nécessaire
if len(bins) > len(avg_superficies_by_bin):
    bins = bins[:-1]

# Visualisation des résultats sous forme d'histogramme
plt.bar(bins, avg_superficies_by_bin, width=10, align='edge')
plt.xlabel('Tranche de 10 ans')
plt.ylabel(f'Superficie Moyenne dans le Département {departement_cible}')
plt.title(f'Superficie Moyenne des Bâtiments dans le Département {departement_cible} par Tranche de 10 ans')
plt.show()

# Fermer la connexion
conn.close()

import sqlite3
import matplotlib.pyplot as plt

# Connexion à la base de données
conn = sqlite3.connect('./permisconstruire.db')

# Code INSEE de la commune spécifique que vous souhaitez visualiser
code_insee_cible = '60057'  # Remplacez 'XXXXX' par le code INSEE de votre commune

# Années spécifiques que vous souhaitez inclure (entre 2010 et 2015)
annee_debut = 2010
annee_fin = 2020

# Exécution de la requête SQL pour extraire les données
query = f"""
    SELECT
        PHR_AT_ANRE AS annee,
        PHR_SUPERFICIE_TERR as superficie
    FROM
        permis_construire
    WHERE
        PHR_AT_ANRE BETWEEN {annee_debut} AND {annee_fin} AND
        COM_COD_INSEE = '{code_insee_cible}' AND
        PHR_SUPERFICIE_TERR IS NOT NULL
    ORDER BY
        PHR_AT_ANRE;
"""

# Exécution de la requête et récupération des résultats
cursor = conn.execute(query)
rows = cursor.fetchall()

# Extraction des données pour le tracé
superficies = [row[1] for row in rows]

# Visualisation des résultats sous forme d'histogramme de densité
plt.hist(superficies, bins=30, density=True, alpha=0.75)
plt.xlabel('Superficie des Logements')
plt.ylabel('Densité')
plt.title(f'Distribution de la Superficie des Logements dans la Commune {code_insee_cible} (2010-2020)')
plt.show()

# Fermer la connexion
conn.close()

import sqlite3
import seaborn as sns
import matplotlib.pyplot as plt

# Connexion à la base de données
conn = sqlite3.connect('./permisconstruire.db')

# Code INSEE de la commune spécifique que vous souhaitez visualiser
code_insee_cible = '21231'

# Années spécifiques que vous souhaitez inclure (entre 2010 et 2015)
annee_debut = 2010
annee_fin = 2020

# Exécution de la requête SQL pour extraire les données
query = f"""
    SELECT
        PHR_AT_ANRE AS annee,
        PHR_SUPERFICIE_TERR as superficie
    FROM
        permis_construire
    WHERE
        PHR_AT_ANRE BETWEEN {annee_debut} AND {annee_fin} AND
        COM_COD_INSEE = '{code_insee_cible}' AND
        PHR_SUPERFICIE_TERR IS NOT NULL
    ORDER BY
        PHR_AT_ANRE;
"""

# Exécution de la requête et récupération des résultats
cursor = conn.execute(query)
rows = cursor.fetchall()

# Extraction des données pour le tracé
superficies = [row[1] for row in rows]

# Visualisation des résultats sous forme de graphique de densité
sns.kdeplot(superficies, fill=True)
plt.xlabel('Superficie des Logements')
plt.ylabel('Densité')
plt.title(f'Distribution de la Superficie des Logements dans la Commune {code_insee_cible} (2010-2020)')
plt.show()

# Fermer la connexion
conn.close()

"""# Algorithmes de détection de bunching



"""

import sqlite3
import matplotlib.pyplot as plt

url_correspondance_communes = './v_commune_2023.csv'
df_correspondance_communes = pd.read_csv(url_correspondance_communes, sep=',')

# Connexion à la base de données
conn = sqlite3.connect('./permisconstruire.db')

# Code INSEE de St Maur des fossés
code_insee_cible = '94068'

# Exécution de la requête SQL pour extraire les données
query = f"""
    SELECT
        PHR_AT_ANRE AS annee,
        PHR_SUPERFICIE_TERR as superficie,
        PHR_AUT_SHON_LOG as surfaceLogementAuth,
        COM_COD_INSEE as CodeCommune
    FROM
        permis_construire
    WHERE
        ((PHR_AT_ANRE >= 2000 AND PHR_AT_ANRE <= 2010) OR PHR_AT_ANRE >= 2014) AND COM_COD_INSEE = '{code_insee_cible}'
    GROUP BY
        PHR_AT_ANRE, superficie
    ORDER BY
        PHR_AT_ANRE, superficie;
"""

# Exécution de la requête et récupération des résultats
df_st_maur = pd.read_sql(query, conn)
df_st_maur = pd.merge(df_st_maur, df_correspondance_communes, left_on='CodeCommune', right_on='COM')
print(df_st_maur)

import numpy as np

df_st_maur['cos_obs'] = df_st_maur['surfaceLogementAuth']/df_st_maur['superficie']
df_st_maur = df_st_maur.replace([np.inf, -np.inf], np.nan)
df_st_maur.dropna(subset=['cos_obs'], inplace=True)

# Créer deux DataFrames pour les deux périodes
df_st_maur_2000_2010 = df_st_maur[(df_st_maur['annee'] >= 2000) & (df_st_maur['annee'] <= 2010)]
df_st_maur_post_2014 = df_st_maur[df_st_maur['annee'] > 2014]

# Définir les paramètres pour les histogrammes
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)  # Premier subplot pour la période 2000-2010
plt.hist(df_st_maur_2000_2010['superficie'], bins=20, color='blue', alpha=0.7)
plt.title('Superficie des permis de Saint Maur des Fossés de 2000 à 2010')
plt.xlabel('Superficie')
plt.ylabel('Fréquence')

plt.subplot(1, 2, 2)  # Deuxième subplot pour la période post-2014
plt.hist(df_st_maur_post_2014['superficie'], bins=20, color='green', alpha=0.7)
plt.title('Superficie des permis de Saint Maur des Fossés après 2014')
plt.xlabel('Superficie')
plt.ylabel('Fréquence')

plt.tight_layout()  # Ajuster le layout pour éviter les chevauchements
plt.show()

"""## Première méthode"""

import numpy as np
import matplotlib.pyplot as plt

def calculer_gradient(pre, post, h1, h2, mu, num_bins):
    G = []
    tabfinal = pd.merge(pre, post, on='bins')
    tabfinal.plot()
    plt.title('Distribution of count_x in tabfinal')
    plt.xlabel('Bins')
    plt.ylabel('Count')
    plt.xlim(0, 200)
    plt.show()
    prints = []
    for l, row in tabfinal.iterrows():
        if l * num_bins - mu < 0:
            continue
        G_l = (np.log(h1[l] + 0.0002) - np.log(h1[l - mu] + 0.0001)) - (np.log(h2[l] + 0.0003) - np.log(h2[l - mu] + 0.0002))
        G.append(G_l)
        if (G_l < -1):
            dic = {
                'superficie': row['bins'].right,
                'bunch': row['count_x'],
                'share': row['count_x'] / tabfinal['count_x'].sum()
            }
            if row['count_x'] > row['count_y']:
                prints.append(dic)
            else:
                pass
    print("Bunch values:")
    print(prints)
    return G


mu = 50

num_bins = int((max(df_st_maur_2000_2010['superficie']) - min(df_st_maur_2000_2010['superficie'])) / 50)
df_st_maur_2000_2010.loc[:, 'bins'] = pd.cut(df_st_maur_2000_2010['superficie'], np.arange(0,100000,60), right=False)
pre = df_st_maur_2000_2010.groupby('bins').size().reset_index(name='count')
df_st_maur_post_2014.loc[:, 'bins'] = pd.cut(df_st_maur_post_2014['superficie'], np.arange(0,100000,60), right=False)
post = df_st_maur_post_2014.groupby('bins').size().reset_index(name='count')

hist_2000_2010, bins_2000_2010 = np.histogram(df_st_maur_2000_2010['superficie'], bins=num_bins)
hist_post_2014, bins_post_2014 = np.histogram(df_st_maur_post_2014['superficie'], bins=num_bins)

gradient = calculer_gradient(pre, post, hist_2000_2010, hist_post_2014, mu, num_bins)

plt.figure(figsize=(8, 6))
plt.plot(gradient, color='red')
plt.title('Gradient entre les deux périodes')
plt.xlabel('Valeur de l')
plt.ylabel('Gradient (G)')
plt.grid(True)
plt.show()

# Définir les paramètres pour les histogrammes
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)  # Premier subplot pour la période 2000-2010
plt.hist(df_st_maur_2000_2010['surfaceLogementAuth'], bins=20, color='blue', alpha=0.7)
plt.title('Superficie des permis de Saint Maur des Fossés de 2000 à 2010')
plt.xlabel('Superficie')
plt.ylabel('Fréquence')

plt.subplot(1, 2, 2)  # Deuxième subplot pour la période post-2014
plt.hist(df_st_maur_post_2014['surfaceLogementAuth'], bins=20, color='green', alpha=0.7)
plt.title('Surface de logement autorisée des permis de Saint Maur des Fossés après 2014')
plt.xlabel('Superficie')
plt.ylabel('Fréquence')

plt.tight_layout()  # Ajuster le layout pour éviter les chevauchements
plt.show()

import numpy as np
import matplotlib.pyplot as plt

def calculer_gradient(pre, post, h1, h2, mu, num_bins):
    G = []
    tabfinal = pd.merge(pre, post, on='bins')
    tabfinal.plot()
    plt.title('Distribution of bunches in tabfinal')
    plt.xlabel('Bunches')
    plt.ylabel('Values')
    plt.xlim(0, 200)
    plt.show()
    prints = []
    for l, row in tabfinal.iterrows():
        if l * num_bins - mu < 0:
            continue
        G_l = (np.log(h1[l] + 0.0002) - np.log(h1[l - mu] + 0.0001)) - (np.log(h2[l] + 0.0003) - np.log(h2[l - mu] + 0.0002))
        G.append(G_l)
        if (G_l < -1):
            dic = {
                'cos': row['bins'].right,
                'bunch': row['count_x'],
                'share': row['count_x'] / tabfinal['count_x'].sum()
            }
            if row['count_x'] > 10:
                prints.append(dic)
            else:
                pass
    print("Bunch values:")
    print(prints)
    return G


mu = 50
df_st_maur_2000_2010['cos'] = df_st_maur_2000_2010['surfaceLogementAuth']/df_st_maur_2000_2010['superficie']
df_st_maur_post_2014['cos'] = df_st_maur_post_2014['surfaceLogementAuth']/df_st_maur_post_2014['superficie']
num_bins = 400
df_st_maur_2000_2010.loc[:, 'bins'] = pd.cut(df_st_maur_2000_2010['cos'], np.arange(0,4,0.01), right=False)
pre = df_st_maur_2000_2010.groupby('bins').size().reset_index(name='count')
df_st_maur_post_2014.loc[:, 'bins'] = pd.cut(df_st_maur_post_2014['cos'], np.arange(0,4,0.01), right=False)
df_st_maur_post_2014=df_st_maur_post_2014.loc[(df_st_maur_post_2014['cos'].notnull()) & (df_st_maur_post_2014['cos']>0) & (df_st_maur_post_2014['cos']<10) ]
df_st_maur_post_2014=df_st_maur_post_2014.replace([np.inf, -np.inf], np.nan)
post = df_st_maur_post_2014.groupby('bins').size().reset_index(name='count')

hist_2000_2010, bins_2000_2010 = np.histogram(df_st_maur_2000_2010['cos'], bins=num_bins)
hist_post_2014, bins_post_2014 = np.histogram(df_st_maur_post_2014['cos'], bins=num_bins)

gradient = calculer_gradient(pre, post, hist_2000_2010, hist_post_2014, mu, num_bins)

plt.figure(figsize=(8, 6))
plt.plot(gradient, color='red')
plt.title('Gradient entre les deux périodes')
plt.xlabel('Valeur de l')
plt.ylabel('Gradient (G)')
plt.grid(True)
plt.show()

"""## Deuxième méthode"""

import geopandas as gpd
import pandas as pd
import sqlite3
from sqlalchemy import create_engine
import numpy as np
from scipy.stats import gaussian_kde
import matplotlib.pyplot as plt

# SQLite connection
conn = sqlite3.connect('./ING3/PFE/permisconstruire.db')

# Read data from the database
def read_data(conn, query):
    return pd.read_sql_query(query, conn)

# KDE estimation
def estimate_kde(data, bw_method=0.1):
    kde = gaussian_kde(data, bw_method=bw_method)
    return kde.evaluate(data)

# Plotting KDE
def plot_kde(values, kde_estimates):
    plt.plot(values, kde_estimates, label='KDE')
    plt.xlabel('Values')
    plt.ylabel('Density')
    plt.title('Kernel Density Estimation')
    plt.legend()
    plt.show()

codgeo = '94068'

# Read data for years 2000 to 2014
query = f"SELECT COM_COD_INSEE, PHR_AUT_SHON_LOG, PHR_SUPERFICIE_TERR FROM permis_construire WHERE COM_COD_INSEE='{codgeo}' AND  TPE_COD_TYPPERMIS='PC' AND PHR_AT_ANRE >= 2000 AND PHR_AT_ANRE <= 2014"
gdf = read_data(conn, query)

# Read data for years 2014 to 2018
gdf_pre = read_data(conn,f"SELECT COM_COD_INSEE, PHR_AUT_SHON_LOG, PHR_SUPERFICIE_TERR FROM permis_construire WHERE COM_COD_INSEE='{codgeo}' AND  TPE_COD_TYPPERMIS='PC' AND PHR_AT_ANRE >= 2014 AND PHR_AT_ANRE <= 2018")

# Calculate 'cos_obs'
gdf['cos_obs'] = gdf['PHR_AUT_SHON_LOG'] / gdf['PHR_SUPERFICIE_TERR']
gdf_pre['cos_obs'] = gdf_pre['PHR_AUT_SHON_LOG'] / gdf_pre['PHR_SUPERFICIE_TERR']
gdf=gdf.loc[(gdf['cos_obs'].notnull()) & (gdf['cos_obs']>0) & (gdf['cos_obs']<10) ]
gdf=gdf.replace([np.inf, -np.inf], np.nan)
values=gdf['cos_obs'].dropna().to_list()
# KDE estimation
kde_estimates = estimate_kde(values)

# Plot KDE
plot_kde(values, kde_estimates)

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import gaussian_kde as kde

gdf['bins'] = pd.cut(gdf['cos_obs'], np.arange(0, 4, 0.01))

# Grouping and calculations for gdf
tab = gdf.groupby('bins').size().reset_index(name='count')
tab['value'] = (tab['bins'].apply(lambda x: x.left).astype(float) + tab['bins'].apply(lambda x: x.right).astype(float)) / 2
tab['density'] = estimate_kde(tab['value'])
tab['grad'] = np.log(tab['density']+0.002) - np.log(tab['density'].shift(1)+0.001)
tab['grad_m1'] = tab['grad'].shift(1)
tab['grad_p1'] = tab['grad'].shift(-1)
tab['cos'] = tab['bins'].apply(lambda x: x.right)
tab['bunch'] = tab['count']
tab['share'] = tab['count'] / tab['count'].sum()

# Filtering for specific conditions
cos = []
for index, row in tab.iterrows():
    if row['grad'] > 1 and row['grad_p1'] * row['grad'] < 0:
        dic = {
            'cos': row['cos'],
            'bunch': row['bunch'],
            'share': row['share']
        }
        if row['cos'] in np.arange(0.1, 2.5, 0.1):
            cos.append(dic)
        else:
            pass

# Grouping and calculations for gdf_pre
gdf_pre['bins'] = pd.cut(gdf_pre['cos_obs'], np.arange(0, 4, 0.01))
tab_pre = gdf_pre.groupby('bins').size().reset_index(name='count')
tab_pre.plot()
tab_pre['geom_pre'] = tab_pre['count']
tab_pre['geom_pre_p1'] = tab_pre['geom_pre'].shift(-1)
tab_pre['grad_pre'] = np.log(tab_pre['geom_pre']+0.002) - np.log(tab_pre['geom_pre_p1']+0.001)
tab_pre['grad_pre_m1'] = tab_pre['grad_pre'].shift(1)

# Filtering for specific conditions in gdf_pre
cospost = []
for index, row in tab_pre.iterrows():
    if row['grad_pre'] > 1 and row['grad_pre_m1'] * row['grad_pre'] < 0:
        dic = {
            'cos': row['bins'].right,
            'bunch': row['geom_pre'],
            'share': row['geom_pre'] / tab_pre['geom_pre'].sum()
        }
        if row['bins'].right in np.arange(0.1, 2.5, 0.1):
            cospost.append(dic)
        else:
            pass

# Merging tab and tab_pre
tabfinal = pd.merge(tab, tab_pre, on='bins')
tabfinal.plot()
# Further filtering on tabfinal
cospost2 = []
for index, row in tabfinal.iterrows():
    grad_diff = np.log(tab['grad']+0.002) - np.log(tab_pre['grad_pre']+0.001)
    if (grad_diff < -1).any():
        dic = {
            'cos': row['bins'].right,
            'bunch': row['geom_pre'],
            'share': row['geom_pre'] / tabfinal['geom_pre'].sum()
        }
        if row['bins'].right in np.arange(0.1, 2.5, 0.1) and row['geom_pre']>0:
            cospost2.append(dic)
        else:
            pass
    if dic['bunch'] > 3:
        print(dic)

"""# Création de la base de données d'entraînement de vérification des l'algorithme"""

import pandas as pd
from sklearn.model_selection import train_test_split
import geopandas as gpd
import pandas as pd
import sqlite3
from sqlalchemy import create_engine
import numpy as np
from scipy.stats import gaussian_kde
import matplotlib.pyplot as plt

# SQLite connection
conn = sqlite3.connect('./ING3/PFE/permisconstruire.db')

# Read data from the database
def read_data(conn, query):
    return pd.read_sql_query(query, conn)

# KDE estimation
def estimate_kde(data, bw_method=0.1):
    kde = gaussian_kde(data, bw_method=bw_method)
    return kde.evaluate(data)

# Plotting KDE
def plot_kde(values, kde_estimates):
    plt.plot(values, kde_estimates, label='KDE')
    plt.xlabel('Values')
    plt.ylabel('Density')
    plt.title('Kernel Density Estimation')
    plt.legend()
    plt.show()

df_determine_cos = pd.read_excel('./ING3/PFE/tab.xlsx')

df_subset = df_determine_cos.loc[:, ['c_coinsee', 'cos']]
elements_uniques = df_subset['c_coinsee'].unique()

def convert_to_nan_or_float(value):
    try:
        return float(value)
    except ValueError:
        return None

df_subset['cos'] = df_subset['cos'].apply(convert_to_nan_or_float)
df_subset['cos'].fillna(0, inplace=True)
df_subset['cos'] = df_subset['cos'].astype(float)

grouped_stats = df_subset.groupby('c_coinsee')['cos'].agg(['sum', 'max', 'mean', 'var', 'std'])
deciles = df_subset.groupby('c_coinsee')['cos'].quantile([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])
grouped_stats = pd.concat([grouped_stats, deciles.unstack(level=1)], axis=1)

grouped_stats = grouped_stats.rename(columns=lambda x: 'decile' + str(int(10*x)) if type(x) == float else x)
grouped_stats = grouped_stats.rename(columns=lambda x: 'reel_cos_' + str(x) if x != 'cos' else x)
grouped_stats.reset_index(inplace=True)
print(grouped_stats)

import random

conn = sqlite3.connect('./ING3/PFE/permisconstruire.db')

codgeoList = grouped_stats['c_coinsee']
print(type(codgeoList))
random.seed(32)
codgeoList_random = random.sample(codgeoList.tolist(), 2)

coslist = []

for i in codgeoList_random:
  # Read data for years 2000 to 2014
  query = f"SELECT COM_COD_INSEE, PHR_AUT_SHON_LOG, PHR_SUPERFICIE_TERR FROM permis_construire WHERE COM_COD_INSEE='{i}' AND  TPE_COD_TYPPERMIS='PC' AND PHR_AT_ANRE >= 2000 AND PHR_AT_ANRE <= 2014"
  gdf = read_data(conn, query)

  # Read data for years 2014 to 2018
  gdf_pre = read_data(conn,f"SELECT COM_COD_INSEE, PHR_AUT_SHON_LOG, PHR_SUPERFICIE_TERR FROM permis_construire WHERE COM_COD_INSEE='{i}' AND  TPE_COD_TYPPERMIS='PC' AND PHR_AT_ANRE >= 2014 AND PHR_AT_ANRE <= 2018")

  # Calculate 'cos_obs'
  gdf['cos_obs'] = gdf['PHR_AUT_SHON_LOG'] / gdf['PHR_SUPERFICIE_TERR']
  gdf_pre['cos_obs'] = gdf_pre['PHR_AUT_SHON_LOG'] / gdf_pre['PHR_SUPERFICIE_TERR']
  gdf=gdf.loc[(gdf['cos_obs'].notnull()) & (gdf['cos_obs']>0) & (gdf['cos_obs']<10) ]
  gdf=gdf.replace([np.inf, -np.inf], np.nan)
  values=gdf['cos_obs'].dropna().to_list()
  # KDE estimation
  if len(values)>1:
    kde_estimates = estimate_kde(values)
  else:
    cospost2 = []
    coslist.append(cospost2)
    continue

  gdf['bins'] = pd.cut(gdf['cos_obs'], np.arange(0, 4, 0.05))

  # Grouping and calculations for gdf
  tab = gdf.groupby('bins').size().reset_index(name='count')
  tab['value'] = (tab['bins'].apply(lambda x: x.left).astype(float) + tab['bins'].apply(lambda x: x.right).astype(float)) / 2
  tab['density'] = estimate_kde(tab['value'])
  tab['grad'] = np.log(tab['density']+0.002) - np.log(tab['density'].shift(1)+0.001)
  tab['grad_m1'] = tab['grad'].shift(1)
  tab['grad_p1'] = tab['grad'].shift(-1)
  tab['cos'] = tab['bins'].apply(lambda x: x.right)
  tab['bunch'] = tab['count']
  tab['share'] = tab['count'] / tab['count'].sum()

  # Filtering for specific conditions
  cos = []
  for index, row in tab.iterrows():
      if row['grad'] > 1 and row['grad_p1'] * row['grad'] < 0:
          dic = {
              'cos': row['cos'],
              'bunch': row['bunch'],
              'share': row['share']
          }
          if row['cos'] in np.arange(0.1, 2.5, 0.1):
              cos.append(dic)
          else:
              pass

  # Grouping and calculations for gdf_pre
  gdf_pre['bins'] = pd.cut(gdf_pre['cos_obs'], np.arange(0, 4, 0.05))
  tab_pre = gdf_pre.groupby('bins').size().reset_index(name='count')
  tab_pre['geom_pre'] = tab_pre['count']
  tab_pre['geom_pre_p1'] = tab_pre['geom_pre'].shift(-1)
  tab_pre['grad_pre'] = np.log(tab_pre['geom_pre']+0.002) - np.log(tab_pre['geom_pre_p1']+0.001)
  tab_pre['grad_pre_m1'] = tab_pre['grad_pre'].shift(1)

  # Filtering for specific conditions in gdf_pre
  cospost = []
  for index, row in tab_pre.iterrows():
      if row['grad_pre'] > 1 and row['grad_pre_m1'] * row['grad_pre'] < 0:
          dic = {
              'cos': row['bins'].right,
              'bunch': row['geom_pre'],
              'share': row['geom_pre'] / tab_pre['geom_pre'].sum()
          }
          if row['bins'].right in np.arange(0.1, 2.5, 0.1):
              cospost.append(dic)
          else:
              pass

  # Merging tab and tab_pre
  tabfinal = pd.merge(tab, tab_pre, on='bins')
  # Further filtering on tabfinal
  cospost2 = []
  for index, row in tabfinal.iterrows():
      grad_diff = np.log(tab['grad']+0.002) - np.log(tab_pre['grad_pre']+0.001)
      if (grad_diff < -1).any():
          dic = {
              'cos': row['bins'].right,
              'bunch': row['geom_pre'],
              'share': row['geom_pre'] / tabfinal['geom_pre'].sum()
          }
          cospost2.append([dic, len(gdf), len(gdf_pre)])

  coslist.append(cospost2)
  print(cospost2)

colonnes = ['c_coinsee',
            'reel_cos_sum', 'reel_cos_max', 'reel_cos_mean', 'reel_cos_var', 'reel_cos_std', 'reel_cos_decile1', 'reel_cos_decile2', 'reel_cos_decile3', 'reel_cos_decile4', 'reel_cos_decile5', 'reel_cos_decile6', 'reel_cos_decile7', 'reel_cos_decile8', 'reel_cos_decile9',
            'permisbefore', 'permisafter',
            'max', 'mean', 'variance', 'sum', 'std_deviation', 'decile1', 'decile2', 'decile3', 'decile4', 'decile5', 'decile6', 'decile7', 'decile8', 'decile9']
df_vide_final = pd.DataFrame(columns=colonnes)

for i in range(0,len(coslist)):
    tmp = []
    for j in range(0,len(coslist[i])):
        tmp.append(coslist[i][j][0]['bunch'])

    if coslist[i] != []:
        permisbefore = (coslist[i][j][1])
        permisafter = (coslist[i][j][2])

    name = codgeoList_random[i]
    filtered_df = grouped_stats[grouped_stats['c_coinsee'] == name]
    mean = np.mean(tmp)
    variance = np.var(tmp)
    std_deviation = np.std(tmp)
    if len(tmp)>0:
        deciles = np.percentile(tmp, np.arange(0, 100, 10))
        max_value = max(tmp)
        sum_value = sum(tmp)
    reel_cos_max = filtered_df['reel_cos_max'].tolist()[0]
    reel_cos_mean = filtered_df['reel_cos_mean'].tolist()[0]
    reel_cos_var = filtered_df['reel_cos_var'].tolist()[0]
    reel_cos_sum = filtered_df['reel_cos_sum'].tolist()[0]
    reel_cos_std = filtered_df['reel_cos_std'].tolist()[0]
    reel_cos_decile1 = filtered_df['reel_cos_decile1'].tolist()[0]
    reel_cos_decile2 = filtered_df['reel_cos_decile2'].tolist()[0]
    reel_cos_decile3 = filtered_df['reel_cos_decile3'].tolist()[0]
    reel_cos_decile4 = filtered_df['reel_cos_decile4'].tolist()[0]
    reel_cos_decile5 = filtered_df['reel_cos_decile5'].tolist()[0]
    reel_cos_decile6 = filtered_df['reel_cos_decile6'].tolist()[0]
    reel_cos_decile7 = filtered_df['reel_cos_decile7'].tolist()[0]
    reel_cos_decile8 = filtered_df['reel_cos_decile8'].tolist()[0]
    reel_cos_decile9 = filtered_df['reel_cos_decile9'].tolist()[0]

    nouvelle_ligne = {'c_coinsee': name, 'reel_cos_sum': reel_cos_sum, 'reel_cos_max': reel_cos_max, 'reel_cos_mean': reel_cos_mean, 'reel_cos_var': reel_cos_var, 'reel_cos_std': reel_cos_std,
                      'reel_cos_decile1': reel_cos_decile1, 'reel_cos_decile2': reel_cos_decile2,'reel_cos_decile3': reel_cos_decile3, 'reel_cos_decile4': reel_cos_decile4, 'reel_cos_decile5': reel_cos_decile5, 'reel_cos_decile6': reel_cos_decile6, 'reel_cos_decile7': reel_cos_decile7, 'reel_cos_decile8': reel_cos_decile8, 'reel_cos_decile9': reel_cos_decile9,
                      'permisbefore': permisbefore, 'permisafter': permisafter,
                      'max': max_value, 'sum': sum_value, 'mean': mean, 'variance': variance,  'std_deviation': std_deviation, 'decile1': deciles[0], 'decile2': deciles[1], 'decile3': deciles[2], 'decile4': deciles[3], 'decile5': deciles[4], 'decile6': deciles[5], 'decile7': deciles[6], 'decile8': deciles[7], 'decile9': deciles[8], 'decile10': deciles[9]}
    df_vide = pd.DataFrame([nouvelle_ligne])
    df_vide_final = pd.concat([df_vide_final, df_vide], ignore_index = True)

df_vide_final = pd.read_csv('./df_vide_final.csv')
df_determine_cos = pd.read_excel('./ING3/PFE/tab.xlsx')

df_subset = df_determine_cos.loc[:, ['c_coinsee', 'codeb', 'cos']]
df_subset['cos'] = df_subset['cos'].fillna(0)

result = pd.merge(df_subset, df_vide_final, on='c_coinsee')

display(result)

"""# Entraînement de modèles"""

df_determine_cos = pd.read_excel('./tab.xlsx')

df_subset = df_determine_cos.loc[:, ['c_coinsee', 'cos']]
elements_uniques = df_subset['c_coinsee'].unique()

def convert_to_nan_or_float(value):
    try:
        return float(value)
    except ValueError:
        return None

df_subset['cos'] = df_subset['cos'].apply(convert_to_nan_or_float)
df_subset['cos'].fillna(0, inplace=True)
df_subset['cos'] = df_subset['cos'].astype(float)

grouped_stats = df_subset.groupby('c_coinsee')['cos'].agg(['sum', 'max', 'mean', 'var', 'std'])
deciles = df_subset.groupby('c_coinsee')['cos'].quantile([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])
grouped_stats = pd.concat([grouped_stats, deciles.unstack(level=1)], axis=1)

grouped_stats = grouped_stats.rename(columns=lambda x: 'decile' + str(int(10*x)) if type(x) == float else x)
grouped_stats = grouped_stats.rename(columns=lambda x: 'reel_cos_' + str(x) if x != 'cos' else x)
grouped_stats.reset_index(inplace=True)
grouped_stats = grouped_stats.dropna()
print(grouped_stats)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score

# Supprimer les lignes avec des valeurs NaN
df = grouped_stats.dropna()

# Séparer les variables indépendantes (X) et la variable dépendante (y)
X = df[['c_coinsee', 'reel_cos_sum', 'reel_cos_decile1', 'reel_cos_decile2', 'reel_cos_decile3', 'reel_cos_decile4', 'reel_cos_decile5', 'reel_cos_decile6','reel_cos_decile7', 'reel_cos_decile8', 'reel_cos_decile9']]
y = df['reel_cos_sum']

# Diviser les données en ensembles d'entraînement et de test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Créer et entraîner le modèle
model = LinearRegression()
model.fit(X_train, y_train)

# Faire des prédictions sur l'ensemble de test
predictions = model.predict(X_test)

# Évaluer les performances du modèle
mse = mean_squared_error(y_test, predictions)
print("Mean Squared Error:", mse)

# Calculer le coefficient de détermination (R²)
r_squared = r2_score(y_test, predictions)
print("Coefficient de détermination (R²) :", r_squared)

"""# Modèle Random Forest"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

# Créer une nouvelle variable cible binaire
grouped_stats['cos_max'] = grouped_stats['reel_cos_max'].apply(lambda x: 1 if x > 0 else 0)

# Diviser les données en features (X) et target (y)
#X = grouped_stats.drop(['reel_cos_max', 'cos_max'], axis=1)
X = grouped_stats[['c_coinsee', 'reel_cos_sum', 'reel_cos_decile1', 'reel_cos_decile2', 'reel_cos_decile3', 'reel_cos_decile4', 'reel_cos_decile5', 'reel_cos_decile6','reel_cos_decile7', 'reel_cos_decile8', 'reel_cos_decile9']]
y = grouped_stats['cos_max']

# Diviser les données en ensembles d'entraînement et de test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialiser le modèle de forêt aléatoire
rf_model = RandomForestClassifier(n_estimators=100, random_state=420)

# Entraîner le modèle
rf_model.fit(X_train, y_train)

# Faire des prédictions sur l'ensemble de test
y_pred = rf_model.predict(X_test)

# Évaluer les performances du modèle
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

# Afficher le rapport de classification
print("Classification Report:")
print(classification_report(y_test, y_pred))

"""# Modèle CNN"""

import pandas as pd
import numpy as np
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from imblearn.over_sampling import RandomOverSampler

# Charger les données
df = grouped_stats.dropna()

# Séparer les variables indépendantes (X) et la variable dépendante (y)
X = df[['c_coinsee', 'reel_cos_sum', 'reel_cos_decile1', 'reel_cos_decile2', 'reel_cos_decile3', 'reel_cos_decile4', 'reel_cos_decile5', 'reel_cos_decile6','reel_cos_decile7', 'reel_cos_decile8', 'reel_cos_decile9']]
y = df['cos_max']

# Appliquer l'oversampling pour équilibrer les classes
oversample = RandomOverSampler(random_state=48498)
X_resampled, y_resampled = oversample.fit_resample(X, y)

# Diviser les données en ensembles d'entraînement et de test
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=48498)

# Normaliser les données
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Redimensionner les données pour les CNN
X_train_cnn = X_train_scaled.reshape(X_train_scaled.shape[0], X_train_scaled.shape[1], 1)
X_test_cnn = X_test_scaled.reshape(X_test_scaled.shape[0], X_test_scaled.shape[1], 1)

# Définir le modèle CNN
model = tf.keras.Sequential([
    tf.keras.layers.Conv1D(32, kernel_size=3, activation='relu', input_shape=(X_train_cnn.shape[1], 1)),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.MaxPooling1D(pool_size=2),
    tf.keras.layers.Conv1D(64, kernel_size=3, activation='relu'),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.MaxPooling1D(pool_size=2),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

# Compiler le modèle
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Entraîner le modèle
history = model.fit(X_train_cnn, y_train, epochs=100, batch_size=8, validation_split=0.2)

# Faire des prédictions sur l'ensemble de test
predictions = (model.predict(X_test_cnn) > 0.5).astype("int32").flatten()

# Calculer la précision
precision = precision_score(y_test, predictions)

# Calculer le rappel
recall = recall_score(y_test, predictions)

# Calculer le score F1
f1 = f1_score(y_test, predictions)

# Calculer la matrice de confusion
conf_matrix = confusion_matrix(y_test, predictions)

print("Précision:", precision)
print("Rappel:", recall)
print("Score F1:", f1)
print("Matrice de confusion:")
print(conf_matrix)

# Connexion à la base de données
conn = sqlite3.connect('./permisconstruire.db')

# Exécution de la requête SQL pour extraire les données
query = f"""
    SELECT
        PHR_AT_ANRE AS annee,
        PHR_SUPERFICIE_TERR as superficie,
        PHR_LI1_SHONCR+PHR_LI1_SHONANT as surface_occupee,
        COM_COD_INSEE as CodeCommune
    FROM
        permis_construire
    WHERE
        PHR_AT_ANRE < 2014
    GROUP BY
        PHR_AT_ANRE, superficie
    ORDER BY
        PHR_AT_ANRE, superficie;
"""

# Exécution de la requête et récupération des résultats
df_avant_2014 = pd.read_sql(query, conn)

import numpy as np

df_avant_2014['reel_cos_sum'] = np.where(df_avant_2014['superficie'] == 0, 0, df_avant_2014['surface_occupee'] / df_avant_2014['superficie'])

deciles_df = df_avant_2014.groupby('CodeCommune')['reel_cos_sum'].quantile([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])

deciles_df = deciles_df.unstack(level=1)
deciles_df.reset_index(inplace=True)
deciles_df.rename(columns={0.1: 'reel_cos_decile1',
                            0.2: 'reel_cos_decile2',
                            0.3: 'reel_cos_decile3',
                            0.4: 'reel_cos_decile4',
                            0.5: 'reel_cos_decile5',
                            0.6: 'reel_cos_decile6',
                            0.7: 'reel_cos_decile7',
                            0.8: 'reel_cos_decile8',
                            0.9: 'reel_cos_decile9',
                            }, inplace=True)

merged_df = df_avant_2014.merge(deciles_df, on='CodeCommune', how='left')

print(merged_df)

df_avant_2014 = df_avant_2014.rename(columns={'CodeCommune': 'c_coinsee'})
df_avant_2014['c_coinsee'] = df_avant_2014['c_coinsee'].str.replace('B', '0')
df_avant_2014['c_coinsee'] = df_avant_2014['c_coinsee'].str.replace('A', '0')
df_avant_2014 = df_avant_2014.dropna()
# Faire des prédictions avec votre modèle RandomForestClassifier
df_avant_2014['predictions'] = rf_model.predict(df_avant_2014[['c_coinsee', 'reel_cos_sum']])

# Vous pouvez ensuite sauvegarder ce DataFrame avec les résultats prédits
df_avant_2014.to_csv('resultats_predictions.csv', index=False)

merged_df = merged_df.rename(columns={'CodeCommune': 'c_coinsee'})
merged_df['c_coinsee'] = merged_df['c_coinsee'].str.replace('B', '0')
merged_df['c_coinsee'] = merged_df['c_coinsee'].str.replace('A', '0')
merged_df = merged_df.dropna()
# Faire des prédictions avec votre modèle RandomForestClassifier
merged_df['predictions'] = rf_model.predict(merged_df[['c_coinsee', 'reel_cos_sum', 'reel_cos_decile1', 'reel_cos_decile2', 'reel_cos_decile3', 'reel_cos_decile4', 'reel_cos_decile5', 'reel_cos_decile6','reel_cos_decile7', 'reel_cos_decile8', 'reel_cos_decile9']])

merged_df.to_csv('./resultats_predictions.csv', index=False)

# Sélectionner les caractéristiques appropriées pour les prédictions
features = merged_df[['c_coinsee', 'reel_cos_sum', 'reel_cos_decile1', 'reel_cos_decile2', 'reel_cos_decile3', 'reel_cos_decile4', 'reel_cos_decile5', 'reel_cos_decile6','reel_cos_decile7', 'reel_cos_decile8', 'reel_cos_decile9']]

# Normaliser les données
features_scaled = scaler.transform(features)

# Redimensionner les données pour les CNN
features_cnn = features_scaled.reshape(features_scaled.shape[0], features_scaled.shape[1], 1)

# Faire des prédictions sur les caractéristiques
predictions_cnn = (model.predict(features_cnn) > 0.5).astype("int32").flatten()

# Ajouter les prédictions au DataFrame
merged_df['predictions_cnn'] = predictions_cnn

# Enregistrer les prédictions dans un fichier CSV
merged_df.to_csv('./resultats_predictions_cnn.csv', index=False)

# Compter le nombre de prédictions de classe 1 et de classe 0
count_1 = np.count_nonzero(predictions_cnn)
count_0 = len(predictions_cnn) - count_1

print("Nombre de prédictions de classe 0:", count_0)
print("Nombre de prédictions de classe 1:", count_1)

TPos,FPos,TNeg,FNeg = 0,0,0,0
for index, row in merged_df.iterrows():
  if row['reel_cos_sum'] > 0:
    if row['predictions'] == 1:
      TPos += 1
    else:
      FNeg += 1
  else:
    if row['predictions'] == 1:
      FPos += 1
    else:
      TNeg += 1
sum = TPos+FPos+TNeg+FNeg
print(TPos/sum,FPos/sum,TNeg/sum,FNeg/sum)
